# Postmortem

## Issue Summary

### Duration:
* Start time: [26/03/2021/1000Hrs] (EAT)
* End time: [26/03/2021/1500Hrs] (EAT)

## Impact:
The infamous gremlins invaded our systems, causing total chaos and a complete disruption to our network services. Users were left disappointed and frustrated as they were unable to access any service. This outage affected 100% of our users reverting operations to manual, turning their smiles into frowns.

## Root Cause:
The root cause of this catastrophic event was traced back to an "air condition(a/c) fail" where the main a/c in the server room failed, creating an extreme rise in temperature, resulting to an automatic shutdown of the servers situation.

## Timeline
[26/03/2021/1000Hrs]: The trouble began when users started reporting that they were unable to access the organization services through the network. Little did we know that it was just the tip of the iceberg.
The incident was quickly escalated to us, the fearless IT team. We rushed to the scene with our capes flapping in the wind.
The server room's diligent monitoring system had already detected a sudden rise in temperature and triggered an alert that switched off the servers.
Initial investigations led us down a peculiar path as we suspected a mischievous squirrel chewing on the network cables.
Armed with determination, and a sense of panic, we scoured the server room, only to find a different type of trouble brewing.
Lo and behold, we discovered that the main a/c in the server room, had decided to take a dip and stopped serving our delicate equipment.
The a/c system had a short circuit and stopped functioning, leaving our servers in a state of shock and disbelief.
We promptly disconnected the power supply, ensuring the safety of our servers.
Our superhero team swiftly cleaned up the mess, bringing in portable-backup a/cs as we waited for reserved engineers to fix the catastrophic malfunction.
Once the a/c equipment were thoroughly setup and the servers had recovered from their temperature-induced trauma, we cautiously restored power and brought all the company's service back to life.
With each passing second, our users rejoiced as they regained access to their beloved service, returning to their happy, productive selves.

## Resolution:
To resolve the issue, our brave IT team took swift action. They disconnected the power supply, brought in backup a/cs, and ensured that the equipment was dry and free from any liquid residue. Once the servers were no longer feeling jittery, power was restored, and the services sprung back to life like a phoenix rising from the ashes.

## Corrective and Preventative Measures

### Improvements/Fixes:
1. Enhanced monitoring and alerts: Strengthen our monitoring system to detect anomalies and alert us promptly in case of any abnormal conditions or server-related disasters.
2. Server room safety protocols: Establish clear guidelines and reminders to prevent system interruptions in the sacred server room. A failure-free zone will ensure uninterrupted service provision and maintain an atmosphere of productivity.
3. Redundancy and backup equipments: Implement redundant power supply and backup systems to minimize the impact of unforeseen incidents and provide a resilient infrastructure for our services.


In conclusion, the services outage was an electrifying event caused by a "a/c failure" in the server room. The heroic IT team tackled the situation with courage and determination, ensuring a complete cleanup and restoration of the service. Moving forward, we will embrace server room safety, redundancy, and enhanced monitoring to prevent any future encounters between the failure and technology. Stay hot, but keep it away from our servers!


And of course, it will never occur again, because we're IT Experts, and we rarely make errors! :wink:
